{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - Which book would you recomend?\n",
    "\n",
    "\n",
    "Jupyter Notebook of the third homework, finished by Lorenzo Petroni and Francesca Casarano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://camo.githubusercontent.com/406644a4e60cd793ef853c47ee30ec2206f3b87081731fb80976824b56417a41/68747470733a2f2f73323938322e7063646e2e636f2f77702d636f6e74656e742f75706c6f6164732f323031352f31322f676f6f6472656164732d65313435373535353432343738302e6a70672e6f7074696d616c2e6a7067)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of all the imported libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import os\n",
    "import regex as re\n",
    "from langdetect import detect\n",
    "import csv\n",
    "import pandas as pd\n",
    "import inflect #convert the numbers into words\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords \n",
    "import nltk\n",
    "import string\n",
    "from pandas import DataFrame\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Get the list of books\n",
    "\n",
    "We start taking the url of the first 300 books, from the list of [**the best book ever**](https://www.goodreads.com/list/show/1.Best_Books_Ever?page=1)\n",
    "\n",
    "We save them in a .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_page = requests.get(\"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=1\")\n",
    "\n",
    "def get_url(url):\n",
    "    req = urllib.request.Request(url)\n",
    "    return urllib.request.urlopen(req)\n",
    "\n",
    "def listToString(s):  \n",
    "    \n",
    "    # initialize an empty string \n",
    "    str1 = \"\\n\"\n",
    "  \n",
    "    return (str1.join(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = 1\n",
    "books_urls = []\n",
    "\n",
    "while (page <= 300):\n",
    "    web_page = requests.get(\"https://www.goodreads.com/list/show/1.Best_Books_Ever?page={}\".format(page))\n",
    "    soup = BeautifulSoup(web_page.content, features=\"lxml\")\n",
    "    \n",
    "    for link in soup.find_all('a', class_='bookTitle', itemprop = 'url', href = True):\n",
    "        url = link.get('href')\n",
    "        url = \"https://www.goodreads.com/\"+url\n",
    "        books_urls.append(url)\n",
    "\n",
    "    str_book_links = listToString(books_urls)\n",
    "    page+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all links found into a file.txt\n",
    "f = open(\"BookLinks.txt\", \"w\")\n",
    "f.write(str_book_links)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl books\n",
    "\n",
    "We download the htlm of each book in the first 300 pages.\n",
    "\n",
    "To store the code, we create some folders, named as \"HTLMSources_Page\" (with number of page), and in each one we saved the htmls of each page as \"Book_Page\" with the number of books and pages.\n",
    "\n",
    "To do this we split the work, we crawled 150 pages for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = 1\n",
    "htmls = []\n",
    "while page <= 300:\n",
    "    \n",
    "    directory = \"HTMLSources_Page\"+str(page)\n",
    "    parent_dir = \"E:\\\\HW3\"\n",
    "    path = os.path.join(parent_dir, directory)\n",
    "    os.mkdir(path)\n",
    "    \n",
    "    web_page = requests.get(\"https://www.goodreads.com/list/show/1.Best_Books_Ever?page={}\".format(page))\n",
    "    soup = BeautifulSoup(web_page.content, features=\"lxml\")\n",
    "    \n",
    "    i = 1\n",
    "    \n",
    "    for link in soup.find_all('a', class_='bookTitle', itemprop = 'url', href = True):\n",
    "        url = link.get('href')\n",
    "        url = \"https://www.goodreads.com/\"+url\n",
    "        html = urllib.request.urlopen(url).read()\n",
    "        html_formatted = html.decode(\"utf-8\")\n",
    "        \n",
    "        namefile = \"Book\"+str(i)+\"_\"+\"Page\"+str(page)\n",
    "        with open(os.path.join(path,namefile), 'w') as f:\n",
    "            f.write(str(html_formatted))\n",
    "            f.close()\n",
    "        i += 1\n",
    "    \n",
    "    page += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages\n",
    "\n",
    "We start to extract the books informations we desire:\n",
    "\n",
    "- Title \n",
    "- Series \n",
    "- Author\n",
    "- Ratings, average stars \n",
    "- Number of givent ratings \n",
    "- Number of reviews \n",
    "- The entire plot \n",
    "- Number of pages \n",
    "- Published \n",
    "- Characters\n",
    "- Setting\n",
    "- Url\n",
    "\n",
    "We save each book as \"article_i.tsv\" with *i* number of article. <b>\n",
    "\n",
    "We use langdetect to choose only the book in english. Because of the different encoding we have to use, we do a first check on book author and title, if they don't exist, we discard the book because it means that this book as a language with non-alphanumeric characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to parse the html pages\n",
    "def parse_pages(parent_dir):\n",
    "    '''\n",
    "    function to parse the downloded pages and take the infos we need\n",
    "    input: parent_dir with the downloaded pages\n",
    "    output: .tsv files\n",
    "    '''\n",
    "    p = 1 #pages, starting from 1\n",
    "    n = 1 #index for the article's name\n",
    "    \n",
    "    while (p<=300):                                 #we start opening the folder related to a specific page\n",
    "        directory = \"HTMLSources_Page\"+str(p)       #directory of the page\n",
    "        path = os.path.join(parent_dir, directory)  #create path\n",
    "        os.chdir(path)\n",
    "        \n",
    "        book_id = 1  #book id per page, starting from 1\n",
    "        \n",
    "        while(book_id <= 100): #we have 100 books per page\n",
    "            book = \"Book\"+str(book_id)+\"_\"+\"Page\"+str(p)\n",
    "            file = open(book, 'r')\n",
    "            html_doc =  BeautifulSoup(file.read(), features=\"lxml\") #open \n",
    "            \n",
    "            #check if the Book Title exists and it's not an empty string \n",
    "            #it appens when the language's book have non-alphanumeric characters--->discard book\n",
    "            \n",
    "            if html_doc.find_all('h1') and len(html_doc.find_all('h1')[0]) > 0: \n",
    "                bookTitle = html_doc.find_all('h1')[0].contents[0]\n",
    "                bookTitle = bookTitle.split('\\n')[1].strip()\n",
    "                article.append(bookTitle)\n",
    "                \n",
    "                #check if the Book Author exists and it's not an empty string \n",
    "                #it appens when the language's book have non-alphanumeric characters--->discard book\n",
    "\n",
    "                if html_doc.find_all('span', itemprop=\"name\") and len(html_doc.find_all('span', itemprop=\"name\")[0]) > 0: \n",
    "                    bookAuthors = html_doc.find_all('span', itemprop=\"name\")[0].contents[0]\n",
    "                    article.append(bookAuthors)\n",
    "                            \n",
    "                    if html_doc.find_all('a', class_=\"greyText\"):\n",
    "                        bookSeries = html_doc.find_all('a', class_=\"greyText\")[0].contents[0]\n",
    "                        if bookSeries == 'Edit Details':    #no book series\n",
    "                            bookSeries = ''                 # we store empty string\n",
    "                            article.append(bookSeries)\n",
    "                        else:\n",
    "                            bookSeries = bookSeries.split('\\n')[1].strip()\n",
    "                            article.append(bookSeries)\n",
    "                    \n",
    "                    if html_doc.find(itemprop=\"ratingValue\"):\n",
    "                        ratingValue = html_doc.find(itemprop=\"ratingValue\").text\n",
    "                        ratingValue = ratingValue.split('\\n')[1].strip()\n",
    "                        article.append(ratingValue)\n",
    "                    if html_doc.find(itemprop=\"ratingCount\"):\n",
    "                        ratingCount = html_doc.find(itemprop=\"ratingCount\").get('content')\n",
    "                        article.append(ratingCount)\n",
    "                        \n",
    "                    if html_doc.find(itemprop=\"reviewCount\"):\n",
    "                        reviewCount = html_doc.find(itemprop=\"reviewCount\").get('content')\n",
    "                        article.append(reviewCount)\n",
    "                    \n",
    "                    if html_doc.find_all('div', id=\"description\"):\n",
    "                        Plot = (html_doc.find(id=\"description\").find_all('span')[-1]).text\n",
    "                        article.append(Plot)\n",
    "                    else:\n",
    "                        Plot = ''\n",
    "                        article.append(Plot)\n",
    "                    \n",
    "                    if (html_doc.find_all('span', {'itemprop' : \"numberOfPages\"})):      \n",
    "                        NumberofPages = html_doc.find_all('span', itemprop=\"numberOfPages\")[0].contents[0]\n",
    "                        NumberofPages = NumberofPages.split(' ')[0]\n",
    "                        article.append(NumberofPages)\n",
    "                    else:\n",
    "                        NumberofPages = ''\n",
    "                        article.append(NumberofPages)\n",
    "                    \n",
    "                    if (html_doc.find_all('div', {'class' : \"row\"})):\n",
    "                        PublishingDate = html_doc.find(id=\"details\").find_all('div')[1].text\n",
    "                        PublishingDate = PublishingDate.split('\\n')[2].strip()\n",
    "                        article.append(PublishingDate)\n",
    "                    else:\n",
    "                        PublishingDate = \"\"    \n",
    "                        article.append(PublishingDate)\n",
    "                    \n",
    "                    if html_doc.find_all('a', {'href' : re.compile(r'/characters')}):\n",
    "                        Characters = ','.join([item.text for item in  html_doc.find_all('a', href = re.compile(r'/characters'))])\n",
    "                        article.append(Characters)\n",
    "                    else:\n",
    "                        Characters = \"\"\n",
    "                        article.append(Characters)\n",
    "                    \n",
    "                    if html_doc.find_all('a', {'href' : re.compile(r'/places')}):\n",
    "                        Setting = ' '.join([item.text for item in  html_doc.find_all('a', href = re.compile(r'/places'))]) \n",
    "                        article.append(Setting)\n",
    "                    else:\n",
    "                        Setting = \"\"\n",
    "                        article.append(Setting)\n",
    "                    \n",
    "                    if html_doc.find('link'):\n",
    "                        Url = html_doc.find('link').get('href')\n",
    "                        article.append(Url)\n",
    "                        \n",
    "                    # we want to save only the books with english plot, so we control with nltk if the language is correct\n",
    "                    try: \n",
    "                        if detect(Plot) =='en':\n",
    "                            path = 'E:\\\\HW3\\\\Articles_test'\n",
    "                            name_article = \"article_\"+str(n)+\".tsv\"\n",
    "                    \n",
    "                            with open(os.path.join(path,name_article), 'w', newline='') as f_output:\n",
    "                                tsv_output = csv.writer(f_output, delimiter='\\t')\n",
    "                                tsv_output.writerow(article)\n",
    "                                n += 1\n",
    "                                article.clear()\n",
    "                    except:\n",
    "                        #go on\n",
    "                        pass  \n",
    "\n",
    "            book_id +=1\n",
    "            \n",
    "        p +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We applied the function to our pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = [] #to create .tsv file\n",
    "parent_dir = \"E:/HW3\"\n",
    "parse_pages(parent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we must pre-process \n",
    "\n",
    "1. Removing stopwords\n",
    "2. Removing punctuation\n",
    "3. Stemming\n",
    "4. Anything else you think it's needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_text(plot):\n",
    "    \"\"\"\n",
    "    Function to pre-process text, returns the text filtered by stopwords, punctuactions and so on \n",
    "    Input: text\n",
    "    Output: list of processed words \n",
    "    \"\"\"  \n",
    "    article_new = \" \"\n",
    "    \n",
    "    #lower case \n",
    "    article_new = plot.lower()\n",
    "    \n",
    "    # Remove numbers \n",
    "    article_new = re.sub(r'\\d+', '', str(article_new))\n",
    "        \n",
    "    #remove genitive\n",
    "    article_new = article_new.replace(\"'\", \" \")\n",
    "\n",
    "    # remove punctuation\n",
    "    article_new = \" \".join(re.split('\\W+', article_new)) #splits string on all non-word characters, joins substrings by single spaces\n",
    "\n",
    "    # remove whitespace from text \n",
    "    article_new =  \" \".join(article_new.split())  #singolo spazio\n",
    "    \n",
    "    ## remove stopwords function\n",
    "    stop_words = set(stopwords.words(\"english\")) \n",
    "    word_tokens = word_tokenize(str(article_new))\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words] \n",
    "    article_new = filtered_text \n",
    "    \n",
    "    ## stem words \n",
    "    porter = PorterStemmer()\n",
    "    article_new = [porter.stem(word) for word in article_new]\n",
    "  \n",
    "    return article_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to create a list containing all the words found in the plots of the articles.\n",
    "\n",
    "We decide to write this function to have the list of word we'll use to create our inverted index<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_words(text_to_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to create the list with all the words in the plots \n",
    "    Input: list \n",
    "    Output: list of words\n",
    "    \"\"\"\n",
    "    \n",
    "    words_list = set()\n",
    "    article_id = 1 #index of the article  \n",
    "\n",
    "    while article_id <= 26572:\n",
    "        article = pd.read_csv('E:\\\\HW3\\\\Articles_test\\\\article_'+str(article_id)+'.tsv',delimiter='\\t',encoding='utf-8') #cp1252 for windows\n",
    "        article = article.columns        \n",
    "        plot = article[6]        #take plot to pre-process\n",
    "        text = preprocessing_text(plot) #pre processed text\n",
    "        text_to_list += text #list with all the words\n",
    "        article_id +=1    \n",
    "        \n",
    "    words_list = set(text_to_list) #list with all the words, without repetitions\n",
    "\n",
    "    return words_list    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the funtion to have the list of words as input for the next step: the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_list = []\n",
    "words_list = list_of_words(text_to_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Conjunctive query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Create your index! \n",
    "\n",
    "We create a file named `vocabulary_dict`, in the format pickle, that maps each word to an integer (`term_id`).\n",
    "\n",
    "We decide to use *pickle* as format to mantain our data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabulary(words_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to create the vocabulary that maps each word to an integer \n",
    "    Input: words_list\n",
    "    Output: vocabulary\n",
    "    \"\"\"\n",
    "    \n",
    "    index = 1\n",
    "    \n",
    "    for word in words_list:\n",
    "        if word not in vocabulary_dict.keys(): \n",
    "            vocabulary_dict[word] = index\n",
    "            index +=1\n",
    "\n",
    "    return vocabulary_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call the function to our list of words of the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_set = []\n",
    "vocabulary_dict = {}\n",
    "\n",
    "voc_dict = vocabulary(text_to_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to store it in a separate file as pikle file. In this way we mantain our data structure also in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('E:\\\\HW3\\\\Files\\\\vocabulary_final.pkl', 'wb') as f:\n",
    "    pickle.dump(voc_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('E:\\\\HW3\\\\Files\\\\vocabulary_final.pkl', 'rb') as f:\n",
    "    vocabulary_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then we create the **Inverted Index**. It is be a `dictionary` of this format:\n",
    "\n",
    "`{\n",
    "term_id_1:[document_1, document_2, document_4],\n",
    "term_id_2:[document_1, document_3, document_5, document_6],\n",
    "...}`\n",
    "where document_i is the id of a document that contains the word.\n",
    "\n",
    "We use the `vocabulary_dict` to take our *term_id* as the new key. The values are the list of document that has the word in their plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_index(vocabulary_dict):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to create the inverted index  \n",
    "    Input: vocabulary_dict\n",
    "    Output: dictionary with Inverted Index\n",
    "    \"\"\"\n",
    "    \n",
    "    doc_list = []\n",
    "    article_id = 1 #index of the article  \n",
    "    \n",
    "    while article_id <= 26572:\n",
    "        article = pd.read_csv('E:\\\\HW3\\\\Articles_test\\\\article_'+str(article_id)+'.tsv',delimiter='\\t',encoding='utf-8') #cp1252 for windows\n",
    "        article = article.columns        \n",
    "        plot = article[6]        #take plot to search the word    \n",
    "        text = preprocessing_text(plot) #pre processed text\n",
    "        text_no_rep = set(text)  #plot without repetitions\n",
    "\n",
    "        for word in vocabulary_dict.keys():\n",
    "            \n",
    "            index = vocabulary_dict[word]\n",
    "            \n",
    "            for el in text_no_rep:\n",
    "                if word == el:\n",
    "                    dictionary_inv_index[index].append('article_'+str(article_id))\n",
    "                    \n",
    "        article_id += 1\n",
    "       \n",
    "    return dictionary_inv_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_inv_index = defaultdict(list)\n",
    "\n",
    "dict_inv_ind = inverted_index(vocabulary_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save dictionary inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('E:\\\\HW3\\\\Files\\\\dictionary_inv_index1.pkl', 'wb') as f:\n",
    "    pickle.dump(dictionary_inv_index, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dictionary inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('E:\\\\HW3\\\\Files\\\\dictionary_inv_index1.pkl', 'rb') as f:\n",
    "    inverted_index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Execute the query\n",
    "\n",
    "Given a query, our Search Engine will return a table with these informations:\n",
    "\n",
    "` bookTitle `\n",
    "` Plot `\n",
    "` Url `\n",
    "\n",
    "We create some function to have the indexes related to the words and to compute the intersection, to have only the documents in which there are all the words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_info_article(query,vocabulary_dict):\n",
    "    '''\n",
    "    Function to get the infos from the vocabulary\n",
    "    input: query and vocabulary_dict\n",
    "    output list of the indices related to the words of the query\n",
    "    '''\n",
    "    input_words = []\n",
    "    \n",
    "    for w in range(len(query)):\n",
    "        if query[w] in vocabulary_dict:\n",
    "            index_term = vocabulary_dict[query[w]]\n",
    "            input_words.append(index_term)\n",
    "            \n",
    "    return input_words # ho i numeri associati alle parole della query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_function(input_words,inverted_index):\n",
    "    \n",
    "    '''\n",
    "    Function to compute the query and search the list of documents in which there are all the words\n",
    "    input: list of words and inverted_index\n",
    "    output list of documents in which there are all the words taken in input\n",
    "    '''\n",
    "    \n",
    "    doc_list = []\n",
    "    final_list = []\n",
    "    \n",
    "    for i in input_words:\n",
    "        i = int(i)\n",
    "        doc= inverted_index[i] #take the list of documents\n",
    "        \n",
    "        if len(doc_list) == 0:\n",
    "            doc_list = doc #base case\n",
    "        else:\n",
    "            final_list = list(set(doc) & set(doc_list))  #intersection\n",
    "            \n",
    "        if len(final_list) == 0:\n",
    "            return doc_list #if I have only one document it'll be in doc_list\n",
    "        else:\n",
    "            return final_list  #list with the documents with all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_result(query, inverted_index,vocabulary_dict):\n",
    "    '''\n",
    "    Function display results of the query\n",
    "    input: query, inverted index, vocabulary\n",
    "    output BookTitle, plot and url per document\n",
    "    '''\n",
    "    list_infos = []\n",
    "    list_to_display = []\n",
    "    \n",
    "    query = preprocessing_text(query)\n",
    "    input_words = query_info_article(query,vocabulary_dict) \n",
    "    final_list = query_function(input_words,inverted_index)\n",
    "    \n",
    "    for d in final_list:\n",
    "        article = pd.read_csv('E:\\\\HW3\\\\Articles_test\\\\'+str(d)+'.tsv',delimiter='\\t',encoding='utf-8') #cp1252 for windows\n",
    "        article = article.columns\n",
    "        title = article[0]       # book title\n",
    "        plot = article[6]        #take plot  \n",
    "        url = article[11]        # take url\n",
    "        \n",
    "        list_infos = [title, plot, url]\n",
    "        \n",
    "        list_to_display.append(list_infos)\n",
    "\n",
    "    return  DataFrame(list_to_display,columns=['Book Title', 'Plot', 'Url'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Search Engine: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Search for:   friends adventure\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book Title</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>David Copperfield</td>\n",
       "      <td>David Copperfield is the story of a young man'...</td>\n",
       "      <td>https://www.goodreads.com/book/show/58696.Davi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Disney in Shadow</td>\n",
       "      <td>When Disney Imagineers installed hologram guid...</td>\n",
       "      <td>https://www.goodreads.com/book/show/6275997-di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You Are Special</td>\n",
       "      <td>Max was interested in helping children underst...</td>\n",
       "      <td>https://www.goodreads.com/book/show/56728.You_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Last Ever After</td>\n",
       "      <td>In the epic next chapter of Soman Chainani’s N...</td>\n",
       "      <td>https://www.goodreads.com/book/show/18004320-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lab Girl</td>\n",
       "      <td>Acclaimed scientist Hope Jahren has built thre...</td>\n",
       "      <td>https://www.goodreads.com/book/show/25733983-l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Ada: Legend of a Healer</td>\n",
       "      <td>No sickness, No injuries, No pain, No limits.I...</td>\n",
       "      <td>https://www.goodreads.com/book/show/10087445-ada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>A Monstrous Regiment of Women</td>\n",
       "      <td>A Monstrous Regiment of Women continues Mary R...</td>\n",
       "      <td>https://www.goodreads.com/book/show/104737.A_M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Comet in Moominland</td>\n",
       "      <td>When Moomintroll learns that a comet will be p...</td>\n",
       "      <td>https://www.goodreads.com/book/show/1003725.Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>That Boy</td>\n",
       "      <td>Fall in love with the boy next door in this sw...</td>\n",
       "      <td>https://www.goodreads.com/book/show/42745424-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Caddie Woodlawn</td>\n",
       "      <td>Caddie Woodlawn is a real adventurer. She'd ra...</td>\n",
       "      <td>https://www.goodreads.com/book/show/205821.Cad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>182 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Book Title  \\\n",
       "0                David Copperfield   \n",
       "1                 Disney in Shadow   \n",
       "2                  You Are Special   \n",
       "3              The Last Ever After   \n",
       "4                         Lab Girl   \n",
       "..                             ...   \n",
       "177        Ada: Legend of a Healer   \n",
       "178  A Monstrous Regiment of Women   \n",
       "179            Comet in Moominland   \n",
       "180                       That Boy   \n",
       "181                Caddie Woodlawn   \n",
       "\n",
       "                                                  Plot  \\\n",
       "0    David Copperfield is the story of a young man'...   \n",
       "1    When Disney Imagineers installed hologram guid...   \n",
       "2    Max was interested in helping children underst...   \n",
       "3    In the epic next chapter of Soman Chainani’s N...   \n",
       "4    Acclaimed scientist Hope Jahren has built thre...   \n",
       "..                                                 ...   \n",
       "177  No sickness, No injuries, No pain, No limits.I...   \n",
       "178  A Monstrous Regiment of Women continues Mary R...   \n",
       "179  When Moomintroll learns that a comet will be p...   \n",
       "180  Fall in love with the boy next door in this sw...   \n",
       "181  Caddie Woodlawn is a real adventurer. She'd ra...   \n",
       "\n",
       "                                                   Url  \n",
       "0    https://www.goodreads.com/book/show/58696.Davi...  \n",
       "1    https://www.goodreads.com/book/show/6275997-di...  \n",
       "2    https://www.goodreads.com/book/show/56728.You_...  \n",
       "3    https://www.goodreads.com/book/show/18004320-t...  \n",
       "4    https://www.goodreads.com/book/show/25733983-l...  \n",
       "..                                                 ...  \n",
       "177   https://www.goodreads.com/book/show/10087445-ada  \n",
       "178  https://www.goodreads.com/book/show/104737.A_M...  \n",
       "179  https://www.goodreads.com/book/show/1003725.Co...  \n",
       "180  https://www.goodreads.com/book/show/42745424-t...  \n",
       "181  https://www.goodreads.com/book/show/205821.Cad...  \n",
       "\n",
       "[182 rows x 3 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = input('Search for:  ')\n",
    "display = display_result(query,inverted_index,vocabulary_dict)\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Conjunctive query & Ranking score\n",
    "For the second search engine, given a query, we want to get the top-k (the choice of k it's up to you!) documents related to the query. In particular:\n",
    "\n",
    "* Find all the documents that contains all the words in the query.\n",
    "* Sort them by their similarity with the query\n",
    "* Return in output k documents, or all the documents with non-zero similarity with the query when the results are less than k. You must use a heap data structure (you can use Python libraries) for maintaining the top-k documents.\n",
    "    \n",
    "    \n",
    "To solve this task, we have to use the tfIdf score, and the Cosine similarity, applied to the plot.\n",
    "\n",
    "First, we create the functions to compute TF, IDF, TFIDF and cosine similarity. We create a second inverted index, similar to the first one, but as *values* we have a list of tuples. The tuple is a couple composed by article and its tfidf values.\n",
    "\n",
    "We'll use it to compute the cosine similarity between query and article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_TF(word,document):\n",
    "    '''\n",
    "    Function to compute TF for a given word in a document\n",
    "    input:  word, document\n",
    "    output: TF\n",
    "    '''\n",
    "    \n",
    "    counter = 0\n",
    "    len_doc = len(document)\n",
    "    \n",
    "    for word_d in document:\n",
    "        if word == word_d:\n",
    "            counter +=1\n",
    "            \n",
    "    tf = counter / len_doc\n",
    "    \n",
    "    return tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_IDF(documents_list):   \n",
    "    '''\n",
    "    Function to compute IDF of a given lists of document in which there is the same word\n",
    "    input:  documents_list\n",
    "    output: IDF\n",
    "    '''\n",
    "    \n",
    "    tot_documents = 26572\n",
    "    len_doc_word = len(documents_list)\n",
    "    idf = math.log(tot_documents / len_doc_word)\n",
    "    \n",
    "    return idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_TFIDF(documents_list,word,document):   \n",
    "    '''\n",
    "    Function to compute TFIDF as the product of TF e IDF\n",
    "    input:  documents_list,word,document\n",
    "    output: IDF\n",
    "    '''\n",
    "    tf = compute_TF(word,document)\n",
    "    idf = compute_IDF(documents_list)\n",
    "    tf_idf = tf * idf\n",
    "    \n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_inverted_index(vocabulary_dict,inverted_index):\n",
    "    \"\"\"\n",
    "    Function to create the second inverted index with tf_idf values per document, related to a specific word \n",
    "    Input: vocabulary_dict\n",
    "    Output: dictionary Inverted Index\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    dictionary_inv_index2 = defaultdict(list)\n",
    "    doc_list2 = []\n",
    "    article_id2 = 1 #index of the article\n",
    "    \n",
    "    while article_id2 <= 26572: \n",
    "\n",
    "        article = pd.read_csv('E:\\\\HW3\\\\Articles_test\\\\article_'+str(article_id2)+'.tsv',delimiter='\\t',encoding='utf-8') #cp1252 for windows\n",
    "        article = article.columns        \n",
    "        plot = article[6]        #take plot to search the word    \n",
    "        plot_processed = preprocessing_text(plot) #pre processed text\n",
    "        \n",
    "        text_no_rep = set(plot_processed)  #plot without repetitions, we'll use it \n",
    "        tupla_valori = ()\n",
    "        \n",
    "        for word in vocabulary_dict.keys():\n",
    "            \n",
    "            index = vocabulary_dict[word]\n",
    "            documents_list = inverted_index[index]\n",
    "            \n",
    "            if word in plot_processed:\n",
    "                tf_idf = compute_TFIDF(documents_list,word,plot_processed)\n",
    "                tupla_valori = ('article_'+str(article_id2)+'.tsv',tf_idf)\n",
    "                \n",
    "                for el in text_no_rep:\n",
    "                    if word == el:\n",
    "                        dictionary_inv_index2[index].append(tupla_valori)\n",
    "            else:\n",
    "                pass\n",
    "                    \n",
    "        article_id2 += 1\n",
    "        \n",
    "    \n",
    "    return dictionary_inv_index2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_inv_index2 = second_inverted_index(vocabulary_dict,inverted_index) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the second dictionary_inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('E:\\\\HW3\\\\Files\\\\inverted_index2_final.pkl', 'wb') as f:\n",
    "    pickle.dump(dictionary_inv_index2, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the second dictionary_inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('E:\\\\HW3\\\\Files\\\\inverted_index2_final.pkl', 'rb') as f:\n",
    "    dictionary_inv_index2 = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "def compute_cosine_similarity(query, document):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to compute cosine similarity\n",
    "    input: query, document\n",
    "    output: cosine similarity between query and document\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(query, document)\n",
    "    norm_a = np.linalg.norm(query)\n",
    "    norm_b = np.linalg.norm(document)\n",
    "    return dot_product / (norm_a * norm_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Engine with similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_second_result(query_2, vocabulary_dict, dictionary_inv_index2,inverted_index):\n",
    "    '''\n",
    "    Function to compute cosine similarity and display the results. Organised data as heap to display top-15\n",
    "    input: query_2, vocabulary_dict, dictionary_inv_index2, inverted_index\n",
    "    output Similarity, BookTitle, plot and url top 15 \n",
    "    '''\n",
    "    list_infos = []\n",
    "    list_to_display = []\n",
    "    \n",
    "    query_2 = preprocessing_text(query_2) #lista di parole\n",
    "    input_words = query_info_article(query_2, vocabulary_dict) #ho una lista con gli indici\n",
    "    final_list = query_function(input_words, inverted_index) #lista dei documenti che hanno tutte le parole della query\n",
    "    \n",
    "    #compute tfidf of the query\n",
    "    tfidf_query_list = []\n",
    "    for i in query_2:\n",
    "        word = i\n",
    "        indice = vocabulary_dict[word]\n",
    "        documents_list = dictionary_inv_index2[indice]\n",
    "        \n",
    "        tfidf_query = compute_TFIDF(documents_list, word, query_2)   #parola = elementi della query, documents_list = ??? final_list query intera = document\n",
    "        tfidf_query_list.append(tfidf_query)\n",
    "        \n",
    "    tfidf_query_arr = np.array(tfidf_query_list) #np array with tfidf of the query\n",
    "    \n",
    "    \n",
    "    dictionary_article_tfidf_values = defaultdict(list) #dictionary in wich we store tfidf for each article (key) as a list of the tfidf of the query's word\n",
    "    \n",
    "    for ind_word in input_words: #For each index related to the query's word taken in input\n",
    "        tfidf_lists = dictionary_inv_index2[ind_word]  #Take the list of tulpe with doc and tfidf from the dictionary\n",
    "        for tuple_article_tfidf in tfidf_lists: \n",
    "            article = tuple_article_tfidf[0]  # take article \n",
    "            if article in final_list:   # If it's one of the article that has all the input words \n",
    "                tfidf_article = tuple_article_tfidf[1] #take tfidf\n",
    "                \n",
    "                dictionary_article_tfidf_values[article].append(tfidf_article) #save tfidf in a list of values with article as key-->list with all the tfidf of the words\n",
    "                \n",
    "    #COMPUTE COSINE SIMILARITY \n",
    "    diz_cosine = {}\n",
    "    \n",
    "    for article_finale in final_list: #for each article with all the input words\n",
    "        list_of_tfidf_art = dictionary_article_tfidf_values[article_finale] # take the list of tfidf related to each words \n",
    "        list_of_tfidf_art_arr = np.array(list_of_tfidf_art) #trasform list of tfidf in np.array\n",
    "        \n",
    "    #cosine similarity between query and article \n",
    "        cosine_similarity = compute_cosine_similarity(tfidf_query_arr, list_of_tfidf_art_arr)  #tfidf_query_arr = query, list_of_tfidf_art_arr = document\n",
    "        diz_cosine[article_finale] = cosine_similarity\n",
    "        \n",
    "        \n",
    "    #Display result \n",
    "\n",
    "    for art in final_list:\n",
    "        article = pd.read_csv('E:\\\\HW3\\\\Articles_test\\\\'+str(art)+'.tsv',delimiter='\\t',encoding='utf-8') #cp1252 for windows\n",
    "        article = article.columns\n",
    "        title = article[0]       # book title\n",
    "        plot = article[6]        #take plot  \n",
    "        url = article[11]        # take url\n",
    "        similarity = diz_cosine[art] #take similarity\n",
    "        list_infos = [similarity, title, plot, url ] \n",
    "        list_to_display.append(list_infos)\n",
    "        \n",
    "    nostro_heap = heapq.nlargest(15, list_to_display)\n",
    "    dataframe_def = DataFrame(nostro_heap,columns=['Similarity', 'Book Title', 'Plot', 'Url'])\n",
    "    \n",
    "    return   dataframe_def[['Book Title', 'Plot', 'Url', 'Similarity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Search Engine: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose kind of plot Forest street\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book Title</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Winter Moon</td>\n",
       "      <td>In Los Angeles, a hot Hollywood director, high...</td>\n",
       "      <td>https://www.goodreads.com/book/show/268559.Win...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wicked</td>\n",
       "      <td>Things are about to get Wicked in New Orleans....</td>\n",
       "      <td>https://www.goodreads.com/book/show/22895264-w...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Gods of Gotham</td>\n",
       "      <td>1845. New York City forms its first police for...</td>\n",
       "      <td>https://www.goodreads.com/book/show/11890816-t...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Manhunt: The 12-Day Chase for Lincoln's Killer</td>\n",
       "      <td>A fascinating tale of murder, intrigue, and be...</td>\n",
       "      <td>https://www.goodreads.com/book/show/146274.Man...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Breath of Snow and Ashes</td>\n",
       "      <td>A Breath of Snow and Ashes continues the extra...</td>\n",
       "      <td>https://www.goodreads.com/book/show/10965.A_Br...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Trees</td>\n",
       "      <td>There came an elastic aftershock of creaks and...</td>\n",
       "      <td>https://www.goodreads.com/book/show/27507117-t...</td>\n",
       "      <td>0.915841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Hidden Life of Trees: What They Feel, How ...</td>\n",
       "      <td>In The Hidden Life of Trees, Peter Wohlleben s...</td>\n",
       "      <td>https://www.goodreads.com/book/show/28256439-t...</td>\n",
       "      <td>0.888750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Book Title  \\\n",
       "0                                        Winter Moon   \n",
       "1                                             Wicked   \n",
       "2                                 The Gods of Gotham   \n",
       "3     Manhunt: The 12-Day Chase for Lincoln's Killer   \n",
       "4                         A Breath of Snow and Ashes   \n",
       "5                                          The Trees   \n",
       "6  The Hidden Life of Trees: What They Feel, How ...   \n",
       "\n",
       "                                                Plot  \\\n",
       "0  In Los Angeles, a hot Hollywood director, high...   \n",
       "1  Things are about to get Wicked in New Orleans....   \n",
       "2  1845. New York City forms its first police for...   \n",
       "3  A fascinating tale of murder, intrigue, and be...   \n",
       "4  A Breath of Snow and Ashes continues the extra...   \n",
       "5  There came an elastic aftershock of creaks and...   \n",
       "6  In The Hidden Life of Trees, Peter Wohlleben s...   \n",
       "\n",
       "                                                 Url  Similarity  \n",
       "0  https://www.goodreads.com/book/show/268559.Win...    1.000000  \n",
       "1  https://www.goodreads.com/book/show/22895264-w...    1.000000  \n",
       "2  https://www.goodreads.com/book/show/11890816-t...    1.000000  \n",
       "3  https://www.goodreads.com/book/show/146274.Man...    1.000000  \n",
       "4  https://www.goodreads.com/book/show/10965.A_Br...    1.000000  \n",
       "5  https://www.goodreads.com/book/show/27507117-t...    0.915841  \n",
       "6  https://www.goodreads.com/book/show/28256439-t...    0.888750  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2 = input('Choose kind of plot')\n",
    "\n",
    "result2 = display_second_result(query_2, vocabulary_dict, dictionary_inv_index2)\n",
    "result2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define a new score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea:** \n",
    "\n",
    "We think that people who are looking for a book take into account its rating. But sometimes this values it's not really accurate, in fact there can be two books with the same rating values but a different number of total ratings. This means that the accurancy of the rating value of the book with an higher number un ratings it's better then the other. <br>\n",
    "\n",
    "The choice of the parameters of the function was made following the logic described here. <br>\n",
    "Our first goal is to take into consideration the RatingValue for each book; However, it would not be possible to define a consistent score without taking into account the RatingCount parameter: a book with a RatingValue of 5 and a high number of RatingCount will have to have a higher score than one with a RatingValue of 5 and a lower RatingCount. <br>\n",
    "In addition to this, we have also decided to give importance to the ReviewCount parameter as we want a book that has been reviewed many times to have a higher score than a book that has received few reviews. <br>\n",
    "Since this last parameter will have to influence the final score much less than the previous two, it has been divided by the total number of documents to be analyzed.\n",
    "Finally, the score obtained is processed by a LOG function in order to obtain lower values that differ less from each other. <br>\n",
    "\n",
    "\n",
    "1. We want to take into account the `ratingValue`, the `ratingCount` and the `reviewCount`\n",
    "    \n",
    "2. For each articles, we compute the *\"new score function\"* as **F = log(ratingValue X ratingCount)  + reviewCount / total_of_documents)** \n",
    "\n",
    "3. We create a  dictionary in which we store as \"key\" the article and as \"value\" the new score related to the book\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_score_function(article):\n",
    "    '''\n",
    "    Function to compute the new score. It takes account of the rating values related to the number of ratings and the reviews\n",
    "    input: article\n",
    "    output: new score\n",
    "    '''\n",
    "    numDoc =26572  \n",
    "    \n",
    "    article = article.columns\n",
    "    ratingValue = article[3]       # ratingValue\n",
    "    ratingCount = article[4]        # ratingCount  \n",
    "    reviewCount = article[5]        # reviewCount\n",
    "    \n",
    "    #We want to exclude the books in which we haven't these infos. It's default score will be zero\n",
    "    \n",
    "    try:\n",
    "        new_score = math.log((float(ratingValue) * float(ratingCount))  + (float(reviewCount) / float(numDoc)))\n",
    "       \n",
    "    except:\n",
    "        return\n",
    "        \n",
    "    return new_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_score_function(vocabulary_dict, inverted_index):\n",
    "    \"\"\"\n",
    "    Function to create the dictionary with the new score  per document\n",
    "    Input: vocabulary_dict, inverted_index\n",
    "    Output: dictionary with document as key and new score as value\n",
    "    \"\"\"\n",
    "\n",
    "    article_id3 = 1 #index of the article\n",
    "    d_score_article = {}\n",
    "    new_score = 0\n",
    "    \n",
    "    while article_id3 <= 26572:\n",
    "        print(article_id3)\n",
    "        article = pd.read_csv('E:\\\\HW3\\\\Articles_test\\\\article_'+str(article_id3)+'.tsv',delimiter='\\t',encoding='utf-8') #cp1252 for windows\n",
    "        \n",
    "        new_score = (new_score_function(article))\n",
    "        \n",
    "        article = article.columns  \n",
    "        plot = article[6]        #take plot to search the word    \n",
    "        plot_processed = preprocessing_text(plot) #pre processed text\n",
    "        text_no_rep = set(plot_processed)  #plot without repetitions\n",
    "        \n",
    "        d_score_article['article_'+str(article_id3)] = new_score\n",
    "        article_id3 +=1\n",
    "        \n",
    "    return d_score_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_score_article1 = dictionary_score_function(vocabulary_dict,inverted_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the new dictionary with the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('E:\\\\HW3\\\\Files\\\\d_score_article.pkl', 'wb') as f:\n",
    "    pickle.dump(d_score_article1, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the new dictionary with the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('E:\\\\HW3\\\\Files\\\\new_score_dict_final.pkl', 'rb') as f:\n",
    "    d_score_article = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_third_result(query_3, vocabulary_dict, d_score_article, inverted_index):\n",
    "    '''\n",
    "    Function to display the results of the query with new score\n",
    "    input: query_3, vocabulary_dict, d_score_article\n",
    "    output BookTitle, plot, url and new score per document\n",
    "    '''\n",
    "\n",
    "    list_infos = []\n",
    "    list_to_display = []\n",
    "    \n",
    "    query_3 = preprocessing_text(query_3)\n",
    "    input_words = query_info_article(query_3, vocabulary_dict) #ho una llista con gli indici\n",
    "    final_list = query_function(input_words, inverted_index)\n",
    "\n",
    "    for art in final_list:\n",
    "        #print(art)\n",
    "        article = pd.read_csv('E:\\\\HW3\\\\Articles_test\\\\'+str(art)+'.tsv',delimiter='\\t',encoding='cp1252')\n",
    "        article = article.columns\n",
    "        title = article[0]       # book title\n",
    "        plot = article[6]        #take plot  \n",
    "        url = article[11]        # take url\n",
    "        \n",
    "        new_score_f = d_score_article[f'{art}'] #take new score\n",
    "        \n",
    "        list_infos = [new_score_f, title, plot, url ] #save infos for each document in a list-->will be a df\n",
    "        list_to_display.append(list_infos)\n",
    "            \n",
    "    #Heap structure\n",
    "    nostro_heap = heapq.nlargest(15, list_to_display)\n",
    "    dataframe_def = DataFrame(nostro_heap,columns=['New Score', 'Book Title', 'Plot', 'Url'])\n",
    "    return   dataframe_def[['Book Title', 'Plot', 'Url', 'New Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose kind of plot Harry Potter!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book Title</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Url</th>\n",
       "      <th>New Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Harry Potter and the Sorcerer's Stone</td>\n",
       "      <td>Harry Potter's life is miserable. His parents ...</td>\n",
       "      <td>https://www.goodreads.com/book/show/3.Harry_Po...</td>\n",
       "      <td>17.274115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>Harry Potter is leaving Privet Drive for the l...</td>\n",
       "      <td>https://www.goodreads.com/book/show/136251.Har...</td>\n",
       "      <td>16.385561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harry Potter and the Prisoner of Azkaban</td>\n",
       "      <td>\"Welcome to the Knight Bus, emergency transpor...</td>\n",
       "      <td>https://www.goodreads.com/book/show/5.Harry_Po...</td>\n",
       "      <td>16.376505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Prisoner of Azkaban</td>\n",
       "      <td>For twelve long years, the dread fortress of A...</td>\n",
       "      <td>https://www.goodreads.com/book/show/5.Harry_Po...</td>\n",
       "      <td>16.376319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Harry Potter and the Chamber of Secrets</td>\n",
       "      <td>Ever since Harry Potter had come home for the ...</td>\n",
       "      <td>https://www.goodreads.com/book/show/15881.Harr...</td>\n",
       "      <td>16.315722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Harry Potter and the Goblet of Fire</td>\n",
       "      <td>Harry Potter is midway through his training as...</td>\n",
       "      <td>https://www.goodreads.com/book/show/6.Harry_Po...</td>\n",
       "      <td>16.294415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Harry Potter and the Half-Blood Prince</td>\n",
       "      <td>The war against Voldemort is not going well; e...</td>\n",
       "      <td>https://www.goodreads.com/book/show/1.Harry_Po...</td>\n",
       "      <td>16.233957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Devil Wears Prada</td>\n",
       "      <td>A delightfully dishy novel about the all-time ...</td>\n",
       "      <td>https://www.goodreads.com/book/show/5139.The_D...</td>\n",
       "      <td>14.899518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Harry Potter and the Cursed Child: Parts One a...</td>\n",
       "      <td>Based on an original new story by J.K. Rowling...</td>\n",
       "      <td>https://www.goodreads.com/book/show/29056083-h...</td>\n",
       "      <td>14.756131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Fantastic Beasts and Where to Find Them</td>\n",
       "      <td>An approved textbook at Hogwarts School of Wit...</td>\n",
       "      <td>https://www.goodreads.com/book/show/41899.Fant...</td>\n",
       "      <td>14.101307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Harry Potter Series Box Set</td>\n",
       "      <td>Over 4000 pages of Harry Potter and his world,...</td>\n",
       "      <td>https://www.goodreads.com/book/show/862041.Har...</td>\n",
       "      <td>13.996709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Quidditch Through the Ages</td>\n",
       "      <td>The most checked-out book in the Hogwarts Libr...</td>\n",
       "      <td>https://www.goodreads.com/book/show/111450.Qui...</td>\n",
       "      <td>13.253497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Fantastic Beasts and Where to Find Them: The O...</td>\n",
       "      <td>J.K. Rowling's screenwriting debut is captured...</td>\n",
       "      <td>https://www.goodreads.com/book/show/29363501-f...</td>\n",
       "      <td>13.173609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Harry Potter: Film Wizardry</td>\n",
       "      <td>Harry Potter: Film Wizardry gives readers a fr...</td>\n",
       "      <td>https://www.goodreads.com/book/show/7952502-ha...</td>\n",
       "      <td>12.477967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The Harry Potter Collection 1-4</td>\n",
       "      <td>The exciting tales of Harry Potter, the young ...</td>\n",
       "      <td>https://www.goodreads.com/book/show/99298.The_...</td>\n",
       "      <td>12.466211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Book Title  \\\n",
       "0               Harry Potter and the Sorcerer's Stone   \n",
       "1                Harry Potter and the Deathly Hallows   \n",
       "2            Harry Potter and the Prisoner of Azkaban   \n",
       "3            Harry Potter and the Prisoner of Azkaban   \n",
       "4             Harry Potter and the Chamber of Secrets   \n",
       "5                 Harry Potter and the Goblet of Fire   \n",
       "6              Harry Potter and the Half-Blood Prince   \n",
       "7                               The Devil Wears Prada   \n",
       "8   Harry Potter and the Cursed Child: Parts One a...   \n",
       "9             Fantastic Beasts and Where to Find Them   \n",
       "10                        Harry Potter Series Box Set   \n",
       "11                         Quidditch Through the Ages   \n",
       "12  Fantastic Beasts and Where to Find Them: The O...   \n",
       "13                        Harry Potter: Film Wizardry   \n",
       "14                    The Harry Potter Collection 1-4   \n",
       "\n",
       "                                                 Plot  \\\n",
       "0   Harry Potter's life is miserable. His parents ...   \n",
       "1   Harry Potter is leaving Privet Drive for the l...   \n",
       "2   \"Welcome to the Knight Bus, emergency transpor...   \n",
       "3   For twelve long years, the dread fortress of A...   \n",
       "4   Ever since Harry Potter had come home for the ...   \n",
       "5   Harry Potter is midway through his training as...   \n",
       "6   The war against Voldemort is not going well; e...   \n",
       "7   A delightfully dishy novel about the all-time ...   \n",
       "8   Based on an original new story by J.K. Rowling...   \n",
       "9   An approved textbook at Hogwarts School of Wit...   \n",
       "10  Over 4000 pages of Harry Potter and his world,...   \n",
       "11  The most checked-out book in the Hogwarts Libr...   \n",
       "12  J.K. Rowling's screenwriting debut is captured...   \n",
       "13  Harry Potter: Film Wizardry gives readers a fr...   \n",
       "14  The exciting tales of Harry Potter, the young ...   \n",
       "\n",
       "                                                  Url  New Score  \n",
       "0   https://www.goodreads.com/book/show/3.Harry_Po...  17.274115  \n",
       "1   https://www.goodreads.com/book/show/136251.Har...  16.385561  \n",
       "2   https://www.goodreads.com/book/show/5.Harry_Po...  16.376505  \n",
       "3   https://www.goodreads.com/book/show/5.Harry_Po...  16.376319  \n",
       "4   https://www.goodreads.com/book/show/15881.Harr...  16.315722  \n",
       "5   https://www.goodreads.com/book/show/6.Harry_Po...  16.294415  \n",
       "6   https://www.goodreads.com/book/show/1.Harry_Po...  16.233957  \n",
       "7   https://www.goodreads.com/book/show/5139.The_D...  14.899518  \n",
       "8   https://www.goodreads.com/book/show/29056083-h...  14.756131  \n",
       "9   https://www.goodreads.com/book/show/41899.Fant...  14.101307  \n",
       "10  https://www.goodreads.com/book/show/862041.Har...  13.996709  \n",
       "11  https://www.goodreads.com/book/show/111450.Qui...  13.253497  \n",
       "12  https://www.goodreads.com/book/show/29363501-f...  13.173609  \n",
       "13  https://www.goodreads.com/book/show/7952502-ha...  12.477967  \n",
       "14  https://www.goodreads.com/book/show/99298.The_...  12.466211  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_3 = input('Choose kind of plot')\n",
    "\n",
    "result3 = display_third_result(query_3, vocabulary_dict, d_score_article)\n",
    "result3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We notice that *\"The Devil Wears Prada\"* appears in the list because it has both \"Harry\" and \"Potter\" *\"From sending the latest, not-yet-in-stores Harry Potter to Miranda’s children in Paris by private jet\"* (cit). It's position it's correct because it is more popular than the other books below in the list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Algorithmic Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given a string written in english capital letters, for example S=\"CADFECEILGJHABNOPSTIRYOEABILCNR.\" You are asked to find the maximum length of a subsequence of characters that is in alfabetical order. For example, here a subsequence of characters in alphabetical order is the \"ACEGJSTY\": \"CADFECEILGJHABNOFPSTIRYOEABILCNR.\" Among all the possible such sequences, you are asked to find the one that is the longest.\n",
    "\n",
    "Define as X[i] = \"the length of the longest sequence of characters in alphabetical order that terminates at the i-th character\". One can prove that\n",
    "\n",
    "X[i] = 1 + max{X[j]; j = 0, ..., i-1, such that S[j]<S[i]}\n",
    "\n",
    "X[i] = 1, if there does not exist such a j.\n",
    "\n",
    "1) Write a recursive program that, given a string, computes the length of the subsequence of maximum length that is in alphabetical order. Try some examples. Are the examples of short strings correct? Can you find examples that your algorithm does not terminate in reasonable time?<br>\n",
    "2) Show that the running time of the algorithm is exponential.<br>\n",
    "3) Write a program that computes the length of the subsequence of maximum length, using dynamic programming.<br>\n",
    "4) What is its runtime complexity?<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Recursive program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 9, 20, 1, 18, 5]\n",
      "Length of the longest ordered substring is 4\n"
     ]
    }
   ],
   "source": [
    "def len_subs_rec(arr, i, n, prev):\n",
    " \n",
    "    if i == n:\n",
    "        return 0\n",
    "\n",
    "    excl = len_subs_rec(arr, i + 1, n, prev)\n",
    "\n",
    "    incl = 0\n",
    "    if arr[i] > prev:\n",
    "        incl = 1 + len_subs_rec(arr, i + 1, n, arr[i])\n",
    "\n",
    "    return max(incl, excl)\n",
    "\n",
    "word = \"ABITARE\"\n",
    "n = len(word)\n",
    "nums = []\n",
    "\n",
    "#convert to lowercase\n",
    "word = word.lower()\n",
    "\n",
    "#convert letters into nums\n",
    "for letter in word:\n",
    "    number = ord(letter) - 96\n",
    "    nums.append(number)\n",
    "\n",
    "print(nums)\n",
    " \n",
    "print(\"Length of the longest ordered substring is\", len_subs_rec(nums, 0, len(nums), float('-inf')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Running time of the recursive program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the running time of our last code increases when \"n\" (the lenght of the string) gets higher.<br>\n",
    "In this solution it is possible to notice how the function for calculating the length of the LIS calls itself twice.<br>\n",
    "We can calculate the execution time as follows: <br>\n",
    "\n",
    "At each cycle the number of letters to be checked decreases by 1, so given $N$ = $numberOfLetters$ we can write that $$T(N) = T(N-1) + T(N-1) + const$$\n",
    "since the function calls itself recursively and the remaining operations have a weight equal to \"const\" for complexity.<br>\n",
    "By iterating the previous formula we obtain that:<br>\n",
    "$T(N) = T(N-1) + T(N-1) + const = 2T(N-1) + const \\le 2T(N-1) = 4T(N-2) = 8T(N-3) = 16T(N-4) = ... = 2^k T(N-k)$\n",
    "So, as we can see, the running time is exponential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Attempt with dynamic programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 9, 20, 1, 18, 5]\n",
      "Length of the longest ordered substring is  4\n"
     ]
    }
   ],
   "source": [
    "def len_subs_dyn(arr): \n",
    "    n = len(arr) \n",
    "  \n",
    "    lis = [1]*n \n",
    "  \n",
    "    for i in range (1 , n):\n",
    "        \n",
    "        for j in range(0 , i): \n",
    "            \n",
    "            if arr[i] > arr[j] and lis[i] < lis[j] + 1 : \n",
    "                lis[i] = lis[j] + 1\n",
    "  \n",
    "    maximum = 0\n",
    "\n",
    "    for i in range(n): \n",
    "        maximum = max(maximum , lis[i]) \n",
    "  \n",
    "    return maximum\n",
    "\n",
    "word = \"ABITARE\"\n",
    "n = len(word)\n",
    "nums = []\n",
    "\n",
    "#convert to lowercase\n",
    "word = word.lower()\n",
    "\n",
    "#convert letters into nums\n",
    "for letter in word:\n",
    "    number = ord(letter) - 96\n",
    "    nums.append(number)\n",
    "\n",
    "print(nums)\n",
    "print(\"Length of the longest ordered substring is \", len_subs_dyn(nums))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running time with dynamic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the longest substring in alphabetical order, the program at each cycle performs one less operation than the previous cycle, discarding the letter just processed.<br>\n",
    "As we can see from the code above we have two \"for\" loops, each one with a complexity weight of \"n\" (because also \"j\" goes to \"i\" that goes to \"n\").<br>\n",
    "So we can compute the running time as:<br> \n",
    "$$T(n) = O(n^2)$$ <br>\n",
    "So the overall complexity has been converted into polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to prove that:\n",
    "\n",
    "$X[i] = 1 + max{X[j]; j = 0, ..., i-1, such that S[j]<S[i]}$\n",
    "\n",
    "$X[i] = 1$, if there does not exist such a j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can prove the provided formula by induction: <br>\n",
    "\n",
    "1) Initial case : proove that it's true for $i = 1$.<br>\n",
    "<br>\n",
    "And of course we have that $X[1] = 1$ <br>\n",
    "<br>\n",
    "2) 2nd step of induction: True for \"$i$\" $\\Rightarrow$ true for \"$i + 1$\". <br>\n",
    "<br>\n",
    "We can observe that, if there exist some \"j\" as specified in the thesis,<br>\n",
    "<br>\n",
    "$X [i + 1] = 1 + X[i]$. <br>\n",
    "<br>\n",
    "Now, thanks to our hypothesis, we have: <br>\n",
    "<br>\n",
    "$X[i + 1] = 1 + [ 1 + max\\{X[j]$ ; $j = 0$,...,$j = i - 1$ such that $S[j] < S[i]\\} ]$ <br>\n",
    "<br>\n",
    "If some \"j\" exists: <br>\n",
    "<br>\n",
    "$X[i+1] = 1 + max\\{X[j]$ ; $j = 0$,...,$j = i$ such that $S[j] < S[i + 1]\\}$ <br>\n",
    "<br>\n",
    "That is our proof-goal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
