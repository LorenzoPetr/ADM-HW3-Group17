{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import os\n",
    "import regex as re\n",
    "from langdetect import detect\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to parse the html pages\n",
    "def parse_pages(parent_dir):\n",
    "\n",
    "    '''\n",
    "    function to parse the downloded pages and take the infos we need\n",
    "    input: parent_dir with the downloaded pages\n",
    "    output: .tsv files\n",
    "    '''\n",
    "    article = [] #to create .tsv file\n",
    "    parent_dir = \"E:/Master-Data Science/ADM/HW3\"\n",
    "    \n",
    "    p = 1        #pages, starting from 1\n",
    "    n = 1\n",
    "    while (p<=150): #we start opening the folder related to a specific page\n",
    "        directory = \"HTMLSources_Page\"+str(p) #directory of the page\n",
    "        path = os.path.join(parent_dir, directory) #create path\n",
    "        os.chdir(path)\n",
    "        \n",
    "        book_id = 1\n",
    "        \n",
    "        #book id per page, starting from 1\n",
    "        while(book_id <= 100):\n",
    "            \n",
    "            book = \"Book\"+str(book_id)+\"_\"+\"Page\"+str(p)\n",
    "            file = open(book, 'r')\n",
    "            html_doc =  BeautifulSoup(file.read(), features=\"lxml\")\n",
    "    \n",
    "            #Infos we want to know\n",
    "    \n",
    "            bookTitle = html_doc.find_all('h1')[0].contents[0]\n",
    "            bookTitle = bookTitle.split('\\n')[1].strip()\n",
    "            article.append(bookTitle)\n",
    "    \n",
    "    \n",
    "            bookAuthors = html_doc.find_all('span', itemprop=\"name\")[0].contents[0]\n",
    "            article.append(bookAuthors)\n",
    "    \n",
    "            bookSeries = html_doc.find_all('a', class_=\"greyText\")[0].contents[0]\n",
    "            #print(bookSeries)\n",
    "            if bookSeries == 'Edit Details':\n",
    "                bookSeries = ''\n",
    "                article.append(bookSeries)\n",
    "            else:\n",
    "                bookSeries = bookSeries.split('\\n')[1].strip()\n",
    "                article.append(bookSeries)\n",
    "            #print(bookSeries)\n",
    "            \n",
    "            \n",
    "    \n",
    "            ratingValue = html_doc.find_all('span', itemprop=\"ratingValue\")[0].contents[0]\n",
    "            ratingValue = ratingValue.split('\\n')[1].strip()\n",
    "            article.append(ratingValue)\n",
    "    \n",
    "            ratingCount = html_doc.find_all('a', href=\"#other_reviews\")[0].contents[2]\n",
    "            ratingCount = ratingCount.split('\\n')[1].strip()\n",
    "            article.append(ratingCount)\n",
    "    \n",
    "            reviewCount = html_doc.find_all('a', href=\"#other_reviews\")[1].contents[2]\n",
    "            reviewCount = reviewCount.split('\\n')[1].strip()\n",
    "            article.append(reviewCount)\n",
    "    \n",
    "            Plot = ' '.join([c for c in html_doc.find_all('div', id=\"description\")[0].contents[1].contents if isinstance(c, str)])\n",
    "            article.append(Plot)\n",
    "            \n",
    "            NumberofPages = html_doc.find_all('span', itemprop=\"numberOfPages\")[0].contents[0]\n",
    "            NumberofPages = NumberofPages.split(' ')[0]\n",
    "            article.append(NumberofPages)\n",
    "    \n",
    "            PublishingDate = html_doc.find_all('div', class_=\"row\")[1].contents[0]\n",
    "            PublishingDate = PublishingDate.split('\\n')[2].strip()\n",
    "            article.append(PublishingDate)\n",
    "    \n",
    "            Characters = ','.join([item.text for item in  html_doc.find_all('a', href= re.compile(r'/characters'))])\n",
    "            article.append(Characters)\n",
    "    \n",
    "            Setting = ' '.join([item.text for item in  html_doc.find_all('a', href= re.compile(r'/places'))]) \n",
    "            article.append(Setting)\n",
    "    \n",
    "            #Url = html_doc.find_all('span', itemprop=\"name\")[0].contents[0]\n",
    "            #print(Url)\n",
    "    \n",
    "            path = 'E:\\\\Master-Data Science\\\\ADM\\\\HW3\\\\Articles'\n",
    "            #name_article = \"articles.tsv\"\n",
    "            name_article = \"article_\"+str(n)+\".tsv\"\n",
    "                #path = 'E:\\\\Master-Data Science\\\\ADM\\\\HW3\\\\Articles'\n",
    "                #name_article = \"article_\"+str(n)+\".tsv\"\n",
    "    \n",
    "            with open(os.path.join(path,name_article), 'w', newline='') as f_output:\n",
    "                tsv_output = csv.writer(f_output, delimiter='\\t')\n",
    "                tsv_output.writerow(article)\n",
    "                n += 1\n",
    "                article.clear()\n",
    "                #article.append()\n",
    "                \n",
    "            book_id +=1\n",
    "        p +=1\n",
    "\n",
    "#####################################################        \n",
    "article = [] #to create .tsv file\n",
    "parent_dir = \"E:/Master-Data Science/ADM/HW3\"\n",
    "parse_pages(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>The Hunger Games</th>\n",
       "      <th>Suzanne Collins</th>\n",
       "      <th>(The Hunger Games #1)</th>\n",
       "      <th>4.33</th>\n",
       "      <th>6,415,388</th>\n",
       "      <th>172,681</th>\n",
       "      <th>In the ruins of a place once known as North America lies the nation of Panem, a shining Capitol surrounded by twelve outlying districts. The Capitol is harsh and cruel and keeps the districts in line by forcing them all to send one boy and one girl between the ages</th>\n",
       "      <th>374</th>\n",
       "      <th>September 14th 2008</th>\n",
       "      <th>Katniss Everdeen,Peeta Mellark,Cato (Hunger Games),Primrose Everdeen,Gale Hawthorne,Effie Trinket,Haymitch Abernathy,Cinna,President Coriolanus Snow,Rue,Flavius,Lavinia (Hunger Games),Marvel,Glimmer,Clove,Foxface,Thresh,Greasy Sae,Madge Undersee,Caesar Flickerman,Claudius Templesmith,Octavia (Hunger Games),Portia (hunger Games)</th>\n",
       "      <th>District 12, Panem Capitol, Panem Panem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [The Hunger Games, Suzanne Collins, (The Hunger Games #1), 4.33, 6,415,388, 172,681, In the ruins of a place once known as North America lies the nation of Panem, a shining Capitol surrounded by twelve outlying districts. The Capitol is harsh and cruel and keeps the districts in line by forcing them all to send one boy and one girl between the ages , 374, September 14th 2008, Katniss Everdeen,Peeta Mellark,Cato (Hunger Games),Primrose Everdeen,Gale Hawthorne,Effie Trinket,Haymitch Abernathy,Cinna,President Coriolanus Snow,Rue,Flavius,Lavinia (Hunger Games),Marvel,Glimmer,Clove,Foxface,Thresh,Greasy Sae,Madge Undersee,Caesar Flickerman,Claudius Templesmith,Octavia (Hunger Games),Portia (hunger Games), District 12, Panem Capitol, Panem Panem]\n",
       "Index: []"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('E:\\\\Master-Data Science\\\\ADM\\\\HW3\\\\Articles\\\\article_1.tsv',delimiter='\\t',encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
